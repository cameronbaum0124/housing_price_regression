{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/cameronbaum0124/housing_price_regression/blob/main/HousingPriceRegressionLab.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Housing prices regression lab"
      ],
      "metadata": {
        "id": "i-QdS7FL6Grw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "This purpose of this lab is to <q>compete</q> in [this variation of Kaggle's House Prices competition](https://marksmath.org/classes/Spring2025MML/AmesRegressionCheck). There's plenty of code to get you started.\n",
        "\n",
        "> A rough description of your mission would be to experiment with various regression functions in SciKit-Learn and to add several more variables in an attempt improve the score.\n",
        "\n",
        "A more precise description of your mission follows at the end of this notebook.\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "PVIuIiHZ6C-J"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Imports"
      ],
      "metadata": {
        "id": "WnQ02bkr62im"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We begin with some common imports."
      ],
      "metadata": {
        "id": "jcVBm2TNM2tj"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wnlAQw1DZjod"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "pd.options.mode.copy_on_write = True\n",
        "\n",
        "from sklearn.linear_model import LinearRegression, RidgeCV, LassoCV\n",
        "from sklearn.preprocessing import StandardScaler, OneHotEncoder, OrdinalEncoder\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.compose import ColumnTransformer\n",
        "from sklearn.impute import SimpleImputer, KNNImputer\n",
        "from sklearn.metrics import mean_squared_error"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# The data"
      ],
      "metadata": {
        "id": "YUcI-paz7GpY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We've got two Data sets, one to *train* with and one to *test* with. Let's import them:"
      ],
      "metadata": {
        "id": "ZKT_DyFG7TpZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train = pd.read_csv('https://marksmath.org/data/reconstructed_train.csv')\n",
        "test = pd.read_csv('https://marksmath.org/data/reconstructed_test.csv')"
      ],
      "metadata": {
        "id": "zThwtHUwZ_e7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "And let's take a quick look at what the data looks like:"
      ],
      "metadata": {
        "id": "M72x5tSS7aum"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(f'{len(train)} observations and {len(train.columns)} features in train')\n",
        "print(f'{len(test)} observations and {len(test.columns)} features in test')\n",
        "train.head()"
      ],
      "metadata": {
        "id": "hS7OVfFwauLx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Well, that's a lot of data. Let's drop all those columns that are missing more than 10 or so values. We'll store the good variables in a list called `good_variables` and display it. You can read a description of the data (that doesn't appear to be 100% accurate) on the [Kaggle competition webpage](https://www.kaggle.com/competitions/house-prices-advanced-regression-techniques/data)."
      ],
      "metadata": {
        "id": "TCEoFCrf8G2v"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def check_most(col_name):\n",
        "  tol = 1400\n",
        "  return len(test[col_name].dropna()) > tol and \\\n",
        "   len(train[col_name].dropna()) > tol\n",
        "good_variables = np.array([c for c in train.columns[2:] if check_most(c) ])\n",
        "print(len(good_variables))\n",
        "good_variables"
      ],
      "metadata": {
        "id": "VoOHSWjrbSJM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "I'm going to illustrate how to deal with one numeric variable, one nominal variable, and one qualitative variable. Let's create a list for each variable type, a list of *all* the variables and examine the resulting data."
      ],
      "metadata": {
        "id": "58Sl98HTdsXt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Store a few variables of interest and of each type in a list:\n",
        "numeric_variables = ['GrLivArea']\n",
        "nominal_variables = ['Neighborhood']\n",
        "qual_variables = ['KitchenQual']\n",
        "\n",
        "# Place one of those into a bigger list and examine:\n",
        "my_variables = np.concatenate([numeric_variables, nominal_variables, qual_variables])\n",
        "train[np.concatenate([my_variables, ['SalePrice']])]"
      ],
      "metadata": {
        "id": "Z03T58macW4A"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "I suppose you can see how we've got one of each type of variable. Typically, ordinal variables are the trickiest to deal with in the code. The reason is that you've got to determine the order yourself. Before you do that you've got to examine the possible values.\n",
        "\n",
        "Here are all possible values of 'KitchenQual':"
      ],
      "metadata": {
        "id": "DrSrFU5GNnCt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "type_check_var = 'KitchenQual'\n",
        "pd.concat([train, test])[type_check_var].unique()"
      ],
      "metadata": {
        "id": "v_L5RYYpPEkQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Note that you've still got to determine a reasonable order.\n",
        "\n",
        "Now, we set up a work flow for each type of variable as follows:\n",
        "\n",
        "- For numeric variables, we\n",
        "  - impute with `KNNImputer`, then\n",
        "  - scale with `StandardScaler`,\n",
        "- For nominal variables, we\n",
        "  - impute with `SimpleImputer`, then\n",
        "  - encode with `OneHotEncoder`,\n",
        "- For Ordinal variables, we\n",
        "  - impute with `SimpleImputer`, then\n",
        "  - encode with `OrdinalEncoder`, then\n",
        "  - scale with `StandardScaler`."
      ],
      "metadata": {
        "id": "boQ0_uRoQE9i"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "categoric_impute = SimpleImputer(strategy=\"most_frequent\")\n",
        "numeric_impute = KNNImputer()\n",
        "\n",
        "nominal_encoder = OneHotEncoder(handle_unknown='ignore')\n",
        "qual_encoder = OrdinalEncoder(categories=[['Ex', 'Gd', 'TA', 'Fa', 'Po']])\n",
        "qual_encoder.fit(np.array(['Ex', 'Gd', 'TA', 'Fa', 'Po']).reshape(-1, 1))\n",
        "\n",
        "scale = StandardScaler(with_mean=False,  with_std=True)"
      ],
      "metadata": {
        "id": "7CXoNkreiMqk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Again, the trickiest thing to deal with the `qual_encoder`, since `KithenQual` is an ordinal variable. For any ordinal variable you choose to add, you need to\n",
        "- Find all the possible values (as we did for `KitchenQual` above),\n",
        "- Determine a reasonable ordering of those values, and\n",
        "- Fit the encoder to that ordering."
      ],
      "metadata": {
        "id": "cVRNQS54AFHQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now, process by transforming all those columns based on type:"
      ],
      "metadata": {
        "id": "VtBAklj1Su3v"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "process = ColumnTransformer(\n",
        "    transformers=[\n",
        "        (\"numerical_impute_and_scale\", Pipeline(\n",
        "            steps = [\n",
        "                ('numeric_impute', numeric_impute),\n",
        "                ('scale', scale)\n",
        "            ]), numeric_variables),\n",
        "        (\"nominal_encode\", Pipeline(steps = [\n",
        "            (\"categoric_impute\", categoric_impute),\n",
        "            (\"nominal_encode\", nominal_encoder)\n",
        "        ]), nominal_variables),\n",
        "       (\"ordinal_impute_and_encode\", Pipeline(steps = [\n",
        "            (\"categoric_impute\", categoric_impute),\n",
        "            (\"ordinal_encode\", qual_encoder),\n",
        "            (\"scale\", scale)\n",
        "        ]), qual_variables)\n",
        "    ]\n",
        ")"
      ],
      "metadata": {
        "id": "OzoMRRgdygjS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "regress = LinearRegression()\n",
        "\n",
        "# Uncomment this next bit of code, if you want to try the ridge.\n",
        "# Probably won't help until you choose a largeish set of variables.\n",
        "\n",
        "# regress = RidgeCV(\n",
        "#     alphas=np.logspace(-1, 1, 100)\n",
        "#   )\n",
        "\n",
        "pipe = Pipeline(steps = [\n",
        "        (\"process\", process),\n",
        "        ('regress', regress)\n",
        "    ]\n",
        ")"
      ],
      "metadata": {
        "id": "ex2sch4Ny3ax"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X = train[my_variables]\n",
        "Y = train.SalePrice[X.index].apply(np.log)\n",
        "pipe.fit(X,Y)"
      ],
      "metadata": {
        "id": "03fz4zKkzIgl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# If you're using RidgeCV, here's the opimized\n",
        "# regularization coefficient:\n",
        "regress.alpha_"
      ],
      "metadata": {
        "id": "dwKiBko3akK4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's score this thing on the train data:"
      ],
      "metadata": {
        "id": "5Ia2QtbEdOMn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "mean_squared_error(Y, pipe.predict(X))**0.5"
      ],
      "metadata": {
        "id": "IZ7MVuzHRBAZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Creating a submission"
      ],
      "metadata": {
        "id": "XAAvtZDLdw31"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We don't have the prices for the test data; we've got to upload our predictions to the [online scoreing tool](https://www.google.com/url?q=https%3A%2F%2Fmarksmath.org%2Fclasses%2FSpring2025MML%2FAmesRegressionCheck) to get the score. Here's how to create a submission file:"
      ],
      "metadata": {
        "id": "bXLC5BgHd0L4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "test['SalePrice'] = pipe.predict(test)\n",
        "submit = test[['Id', 'SalePrice']]\n",
        "submit.loc[:, 'SalePrice'] = submit.SalePrice.apply(np.exp) # Gotta scale back now\n",
        "\n",
        "# Rounding to the nearest 1000 seems to help a little bit.\n",
        "# I suppose that's because the orginal prices are rounded\n",
        "submit.loc[:, 'SalePrice'] = submit.SalePrice.apply(lambda x: 1000*round(x/1000))\n",
        "submit.to_csv('housing_predictions_demo.csv', index=False)\n",
        "submit"
      ],
      "metadata": {
        "id": "YqRewCRi1Pqd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Your task:**\n",
        "\n",
        "Your task is to fiddle with the variables to see if you can improve the score. My score [should be *about* 0.13856](https://www.kaggle.com/competitions/house-prices-advanced-regression-techniques/leaderboard?search=mcclure)."
      ],
      "metadata": {
        "id": "UGOk-dIyeMgQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## LASSOing relevant variables"
      ],
      "metadata": {
        "id": "yN7jhLzlWZev"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We can use `LassoCV` to help us determine some particularly relevant variables. This is easiest with numerical variables, so I'm going to stick with that. While it can be done with categorical variables as well, there are considerable complications. Nominal variables that are encoded with `OneHotEncoder` are a particular pain.\n",
        "\n",
        "There are 33 numeric variables in our `good_variable` list so that's more than enough to make an interesting example.  Here's the whole process in one cell:"
      ],
      "metadata": {
        "id": "yS_JRnJrWjJS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Grab the numeric variables:\n",
        "all_numeric_variables = train.select_dtypes(include=[\"int64\", 'float64']).columns\n",
        "all_numeric_variables = [c for c in all_numeric_variables if c in good_variables]\n",
        "\n",
        "# Build a simple pre-preocessor:\n",
        "process_just_numeric = ColumnTransformer(\n",
        "    transformers=[\n",
        "        (\"numerical_impute_and_scale\", Pipeline(\n",
        "            steps = [\n",
        "                ('numeric_impute', numeric_impute),\n",
        "                ('scale', scale)\n",
        "            ]), all_numeric_variables)\n",
        "    ]\n",
        ")\n",
        "\n",
        "# Regress and fit the cross-valideated Lasso!\n",
        "regress = LassoCV(\n",
        "    alphas=np.logspace(-6, 6, 100)\n",
        "  )\n",
        "pipe_just_numeric = Pipeline(steps = [\n",
        "        (\"process\", process_just_numeric),\n",
        "        ('regress', regress)\n",
        "    ]\n",
        ")\n",
        "X = train[all_numeric_variables]\n",
        "Y = train.SalePrice[X.index].apply(np.log)\n",
        "pipe_just_numeric.fit(X,Y)\n",
        "\n",
        "# Display the variables together with their coefficients\n",
        "# in descending order.\n",
        "stats = []\n",
        "for i in range(len(X.columns)):\n",
        "    stats.append({'stat': X.columns[i], 'coeff': regress.coef_[i]})\n",
        "stats_df = pd.DataFrame(stats)\n",
        "stats_df.sort_values('coeff', ascending=False)"
      ],
      "metadata": {
        "id": "hUcdWsuG1pwG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "If you're going to add some numeric variables, it might make sense to start near the top of that list!"
      ],
      "metadata": {
        "id": "UBDgHKZ-YewD"
      }
    }
  ]
}